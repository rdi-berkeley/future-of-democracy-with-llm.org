<!DOCTYPE html>
<html lang="en" class="h-full">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>LLMs' Potential Influences on Our Democracy: Challenges and Opportunities</title>
    <meta name="description" content="">
    <meta name="keywords" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com/">
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="">
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400&amp;display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Serif:wght@400;700&#038;display=swap"
        rel="stylesheet">
    <link href="./assets/css/css2" rel="stylesheet">

    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700;900&display=swap" rel="stylesheet">

    <!-- Twitter Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta property="twitter:domain" content="democracy-with-llm.org">
    <meta property="twitter:url" content="https://future-of-democracy-with-llm.org/">
    <meta name="twitter:title" content="LLMs' Potential Influences on Our Democracy: Challenges and Opportunities">
    <meta name="twitter:description" content="">
    <meta name="twitter:image" content="https://future-of-democracy-with-llm.org/assets/image2.png">

    <!--
    <script src="./assets/js/splide.min.js"></script>
    <link rel="stylesheet" href="/assets/css/splide.min.css">
    -->
    <script src="./assets/js/61c2f15e92f56eaa354c18452db280ac.js"></script>
    <link rel="stylesheet" href="./assets/css/style.css">
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            new Splide('.splide').mount();
        });
    </script>
</head>

<body> <!--style="background-color: #fffbe6;">

    <!-- <div style="background-color: #003262">
        <div class="constraint">
            <div id="header">
                <div id="logo-container" style="padding-top: 1rem;">
                    <a href="./" aria-label="Berkeley Logo">
                        <img src="./assets/logo-berkeley-gold.svg" width="200" aria-label="Berkeley Logo">
                    </a>
                </div>
            </div>
        </div>
    </div> -->

    <div> <!-- style="border-bottom:3px solid #0066cc;"-->
        <div class="constraint">
            <div style="text-align: center;">
                <h1>
                    LLMs' Potential Influences on Our Democracy: <br>Challenges and Opportunities</br>
                </h1>
                <p>
                    <strong>Yujin&nbsp;Potter<sup>1</sup>, Yejin&nbsp;Choi<sup>2</sup>,
                    David&nbsp;Rand<sup>3</sup>, Dawn&nbsp;Song<sup>1</sup></strong>
                </p>
                <p>
                    <sup>1</sup>UC&nbsp;Berkeley
                    <sup>2</sup>University&nbsp;of&nbsp;Washington
                    <sup>3</sup>MIT 
                    <!-- <br> -->
                    <!-- <sup>*</sup>Corresponding author  </p>-->
                
                <!--<p style="font-size: 0.85em;">
                    <sup>*</sup>Several authors have unlisted affiliations in addition to their listed university
                    affiliation. This piece solely reflects the authors' personal views and not those of any affiliated
                    organizations, including the listed universities.
                </p> -->
            </div>
        </div>
    </div>

    <style>
	.summary {
            max-width: 800px;
            margin: 0 auto 40px auto;
            padding: 20px 25px; /* Reduced vertical padding */
            background-color: #f8f9fa;
            border-left: 4px solid #0066cc;
            border-radius: 4px;
        }
        .summary h2 {
            margin-top: 0;
            margin-bottom: 10px; /* Reduced space after heading */
            color: #0066cc;
            font-size: 1.3rem; /* Slightly smaller heading */
        }
        .summary p {
            margin: 0; /* Remove default paragraph margins */
            font-size: 1.1rem; /* Slightly smaller text */
            line-height: 1.5; /* Slightly tighter line height */
        }
	.main-content {
            margin: 45px auto 0;
            padding: 0 10px;
            font-size: 18px;
            line-height: 26px;
        }
		
        .main-text div {
	    max-width: 1000px;
            margin-left: auto;
            margin-right: auto;
            margin-top: 45px;
            font-size: 18px;
            line-height: 26px;
	    padding: 0 10px;
        }

	@media (max-width: 768px) {
            .main-text div {
			max-width: calc(100% - 6vw);
			margin-left: auto;
			margin-right: auto;
			font-size: 18px;          
			line-height: 26px;        /* Increased from 24px */
		}
		
		.summary p {
			font-size: 1.2rem;       /* Increased from 0.95rem */
		}
		
		h1 {
			font-size: 2rem;          /* Increased from 1.8rem */
		}
		
		h3 {
			font-size: 1.4rem;        /* Increased from 1.2rem */
		}
        }
    </style>

    <div class="summary">
       <h2>Summary</h2>
       <p>A growing literature [<a href="https://arxiv.org/pdf/2410.24190">Potter et al. 2024</a>, <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0306621">Rozado 2024</a>, <a href="https://aclanthology.org/2023.acl-long.656/">Feng et al. 2023</a>, <a href="https://proceedings.mlr.press/v202/santurkar23a/santurkar23a.pdf">Santurkar et al. 2023</a>, <a href="https://arxiv.org/pdf/2301.01768">Hartmann et al. 2023</a>, <a href="https://arxiv.org/pdf/2410.09978">Vijay et al. 2024</a>] shows that LLMs exhibit left political leaning on a range of issues. This has sparked active discussion about LLMs' potential influences on political discourse and democratic processes. Recent papers have taken initial steps forward in exploring this question. For example, <a href="https://arxiv.org/pdf/2410.24190">Potter et al. (2024)</a> showed that interactions with LLMs can sway voters towards the Democratic candidate, even without LLMs being instructed to persuade voters. <a href="https://arxiv.org/pdf/2410.06415">Fisher et al. (2024)</a> revealed that LLMs with radical political leanings can inject their political views into users' decisions on less sensitive political topics. However, these studies only examined the immediate effects of LLMs, leaving the question of their long-term effects open. On the other hand, <a href="https://www.science.org/doi/abs/10.1126/science.adq1814">Costello et al. (2024)</a> demonstrated how LLM conversations can be used positively to durably reduce users' conspiracy beliefs. These findings highlight the importance of further studies on how LLMs may influence our democracy. Many crucial questions currently remain unanswered: What factors cause LLMs' left leaning? What goals should we set for these models? For example, should we pursue political neutrality in AI systems or some other goal(s), such as accuracy and/or pluralistic values? Is political neutrality even possible? This article discusses the path forward and proposes future research questions in four broad areas: (1) evaluation of LLM political leanings, (2) understanding LLMs’ influences on our democracy, (3) better policy frameworks for AI development, and (4) technical solutions to mitigate political leanings. As LLMs become increasingly integrated into society, continued investigation of how LLMs will reshape democracy is essential to maximize their benefits while minimizing risks to democratic processes.</p>
    </div>
    <div class="main-text">
        <div class="features-wrapper constraint">
            <div class="row">
                <div class="col-1"></div>
                <div class="col-12 mt-5 paragraph-spacing">
		    <p>As large language models (LLMs) continue to advance at a remarkable pace, understanding their societal implications has become increasingly vital. In particular, the potential influence of LLMs on political discourse has emerged as a critical area of study. Researchers in various fields such as economics, philosophy, and law have also recently voiced the importance of research in this area [<a href="https://www.amazon.com/Digitalist-Papers-Artificial-Intelligence-Democracy/dp/B0DFMH1C11">Brynjolfsson et al. 2024</a>]. Recent papers [<a href="https://arxiv.org/pdf/2410.24190">Potter et al. 2024</a>, <a href="https://arxiv.org/pdf/2410.06415">Fisher et al. 2024</a>, <a href="https://www.science.org/doi/abs/10.1126/science.adq1814">Costello et al. 2024</a>] have taken initial steps toward investigating how LLMs can influence users’ political beliefs. These findings raise important questions that need further exploration. We identify key research directions to ensure LLMs can constructively contribute to democratic processes and society.</p>
                    <p><h3>LLMs’ Political Leanings and Their Effects on Users</h3>
					<strong>LLMs’ Left-of-Center Outputs</strong></p>
					<p>Much recent literature [<a href="https://arxiv.org/pdf/2410.24190">Potter et al. 2024</a>, <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0306621">Rozado 2024</a>, <a href="https://aclanthology.org/2023.acl-long.656/">Feng et al. 2023</a>, <a href="https://proceedings.mlr.press/v202/santurkar23a/santurkar23a.pdf">Santurkar et al. 2023</a>, <a href="https://arxiv.org/pdf/2301.01768">Hartmann et al. 2023</a>, <a href="https://arxiv.org/pdf/2410.09978">Vijay et al. 2024</a>] has shown that LLMs exhibit left-leaning. For instance, <a href="https://arxiv.org/pdf/2410.24190">Potter et al. (2024)</a> examined LLMs' political leanings in three scenarios: (1) a voting simulation, (2) LLMs' comments on candidate policies, and (3) interactive political discourse with users in the context of the U.S. presidential election. These experiments consistently demonstrated LLMs' preference for the Democratic nominee (Joseph R. Biden, during the study period) and his policies over the Republican nominee (Donald J. Trump). In particular, despite the well-known tendency of LLMs to exhibit sycophancy bias, LLMs exhibited this political leaning regardless of their human interlocutors' political stances. Many studies [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0306621">Rozado 2024</a>, <a href="https://aclanthology.org/2023.acl-long.656/">Feng et al. 2023</a>, <a href="https://proceedings.mlr.press/v202/santurkar23a/santurkar23a.pdf">Santurkar et al. 2023</a>, <a href="https://arxiv.org/pdf/2301.01768">Hartmann et al. 2023</a>] also revealed LLMs’ left-leaning on various issues, using multiple-choice surveys and questionnaires widely employed in social science.
					</p>
					<p>Additionally, recent studies have documented the manifestation of LLMs’ left-leaning in various applications. For example, <a href="https://arxiv.org/pdf/2410.09978">Vijay et al. (2024)</a> showed that LLM-generated news summaries tend to highlight liberal perspectives. Similarly, <a href="https://aclanthology.org/2023.acl-long.656/">Feng et al. (2023)</a> showed that this left-leaning tendency affects their ability to detect misinformation and hate speech, with varying sensitivity based on the political orientation of source materials. These studies demonstrate LLMs’ consistent political leanings across diverse contexts.</p>
                    <p><strong>LLMs’ Influence on Users’ Political Stances</strong></p>
					<p>Given these documented left-leaning tendencies, society has begun discussing LLMs' potential impact on users' political views and democratic processes more broadly. However, their effect on democracy, particularly how interactions with LLMs might shape users’ political perspectives, remains largely unexplored. Recent research has examined whether direct LLM interactions can influence users’ political viewpoints.</p>
                    <p><a href="https://arxiv.org/pdf/2410.24190">Potter et al. (2024)</a> demonstrated that even brief interactions with LLMs (which were simply asked to provide answers and comments regarding Biden and Trump's policies without any persuasive intent) could shift voters' choices in the presidential election context, where individuals typically hold firm opinions.  <a href="https://arxiv.org/pdf/2410.06415">Fisher et al. (2024)</a> demonstrated that LLMs programmed with extreme right or left-wing viewpoints can influence human decisions on unfamiliar political topics through brief conversations, illustrating how LLMs' explicit political leanings can affect users’ political stances. Additional research has explored intentional persuasion by LLMs designed to promote specific political positions or spread misinformation [<a href="https://www.anthropic.com/news/measuring-model-persuasiveness">Anthropic persuasion report 2024</a>, <a href="https://academic.oup.com/pnasnexus/article/3/2/pgae034/7610937">Goldstein et al. 2024</a>]. Notably, <a href="https://www.science.org/doi/abs/10.1126/science.adq1814">Costello et al. (2024)</a> showed that interactions with LLMs can persuade people to reduce their belief in conspiracies, including political conspiracies (e.g. related to election fraud and COVID-19), suggesting potential positive applications for such persuasive capabilities. These studies describe how LLMs could influence our democracy, highlighting the necessity of further research. However, several caveats apply to these studies. For example, because these experiments were conducted in controlled settings, the findings may not generalize to real-world applications. Moreover, these findings’ robustness and the influence of various factors, including prompt variations and experiment timing, warrant additional investigation.</p>
                    <p><h3>The Path Forward and Future Research Questions</h3> The observed left-leaning tendencies of LLMs and their influence on users’ political perspectives raise critical questions that society must address. We highlight four key areas requiring further investigation by both researchers and society at large.</p>
					
					<p>
					<strong>First, we need more comprehensive model evaluation and analysis.</strong>
						<ul>
                        <li><strong>When and how do LLMs manifest their political leaning?</strong> While many studies have identified LLMs' left-leaning tendencies, their responses can vary based on prompts and context. For example, <a href="https://arxiv.org/pdf/2402.16786">Röttger et al. (2024)</a> and <a href="https://arxiv.org/pdf/2402.17649">Ceron et al. (2024)</a> demonstrated that LLMs' political answers can shift depending on question framing. This suggests the need for examining LLMs’ political leanings across diverse scenarios. Additionally, as LLM applications become more widespread, understanding which specific use cases might exhibit these left-leaning tendencies becomes crucial.</li>
                        <li><strong>What causes LLMs’ left-leaning tendencies?</strong> The complexity of model development makes identifying precise reasons why political stances emerge challenging. Training data composition likely plays a role, as it primarily consists of modern web content, that may skew liberal. Additionally, these studies [<a href="https://arxiv.org/pdf/2410.24190">Potter et al. 2024</a>, <a href="https://proceedings.mlr.press/v202/santurkar23a.html">Santurkar et al. 2023</a>, <a href="https://arxiv.org/pdf/2402.05070">Sorensen et al. 2024</a>] indicate that post-training processes may have amplified these political leanings. Specifically, instruction-tuned models show stronger left-leaning tendencies compared to their base versions, suggesting that current post-training methods increase political leanings. However, the specific aspects driving this amplification remain unclear and require further investigation. One possibility is that political orientation may be confounded with other relevant features [<a href="https://www.nature.com/articles/s41586-024-07942-8">Mosleh et al. 2024</a>, <a href="https://arxiv.org/pdf/2409.05283">Fulay et al. 2024</a>]. For example, <a href="https://arxiv.org/pdf/2409.05283">Fulay et al. (2024)</a> observed a left-leaning orientation of models trained for truthfulness, on some specific political topics such as climate change. This suggests that even apolitical fine-tuning efforts may inadvertently lead to responses that align more closely with liberal positions on certain issues. This may stem in part from truthfulness-related training data including scientific facts being more aligned with liberal positions for specific issues like climate change [<a href="https://psycnet.apa.org/record/2022-86433-001">Pennycook et al. 2023</a>, <a href="https://en.wikipedia.org/wiki/False_balance">False balance</a>]. Quantifying these contributing factors represents an essential step toward understanding the models' political leanings.</li>
						</ul></p>
					<p>
						<strong>Second, we need to deepen our understanding of LLMs' influence on users and their impact on democracy.</strong>
						<ul>
                        <li><strong>When and how can LLMs shape users' views and affect our democratic processes?</strong> While recent papers [<a href="https://arxiv.org/pdf/2410.24190">Potter et al. 2024</a>, <a href="https://arxiv.org/pdf/2410.06415">Fisher et al. 2024</a>, <a href="https://www.science.org/doi/abs/10.1126/science.adq1814">Costello et al. 2024</a>] take a step forward in investigating this question, several crucial questions remain unresolved. Foremost, how effectively will these findings translate to real-world scenarios? For example, the observed candidate-leaning changes of voters following the LLM interaction in <a href="https://arxiv.org/pdf/2410.24190">Potter et al. (2024)</a> might not translate to actual voting behavior. Currently, no research has assessed LLMs' impact on users’ political perspectives in the wild. We call for extensive field experiments to evaluate these effects more accurately. Second, although significant changes in study participants' political stance were observed after LLM intervention [<a href="https://arxiv.org/pdf/2410.24190">Potter et al. 2024</a>, <a href="https://arxiv.org/pdf/2410.06415">Fisher et al. 2024</a>], these effects might be time-dependent. For example, political science literature [<a href="https://www.cambridge.org/core/journals/american-political-science-review/article/abs/minimal-persuasive-effects-of-campaign-contact-in-general-elections-evidence-from-49-field-experiments/753665A313C4AB433DBF7110299B7433">Kalla et al. 2017</a>] suggests that even successful political persuasion tends to diminish over time. Nevertheless, <a href="https://www.science.org/doi/abs/10.1126/science.adq1814">Costello et al. (2024)</a> demonstrated significant long-term reductions in users' conspiracy beliefs. Whether and when LLM-induced political shifts persist presents another compelling avenue for future research. Furthermore, we must explore how LLM deployment in various contexts will affect users. Understanding which applications carry greater or lesser influence on users will prove essential. Additional research should examine the conditions under which LLMs most significantly influence public opinion. </li>
                        <li><strong>Why are LLMs able to shape users' views and affect our democratic processes?</strong> The mechanisms behind LLM’s effects on participants’ political leanings remain largely undefined. LLM interactions encompass numerous characteristics beyond their inferred political leanings, including elements like helpfulness and engagement style. Understanding their influence requires exploring how each factor might affect users’ political perspectives and determining their relative impact. For example, <a href="https://arxiv.org/pdf/2410.06415">Fisher et al. 2024</a> discovered that LLMs expressing extreme political views effectively swayed user opinions on unfamiliar political topics. Similarly, we need to investigate which other aspects (like truthfulness or helpfulness) of LLM interactions can influence people's viewpoints. Disentangling these various factors to identify the precise mechanisms of LLMs' political influence presents an ongoing challenge requiring sustained research.</li>
						<li><strong>Should we always view LLM influence on democracy as problematic?</strong> While some might argue that any LLM influence on human perspectives is inherently concerning, recent work suggests these systems could enhance democratic processes. <a href="https://www.science.org/doi/abs/10.1126/science.adq1814">Costello et al. 2024</a> demonstrate that LLMs can effectively reduce individuals' belief in conspiracy theories. <a href="https://www.science.org/doi/10.1126/science.adq2852">Tessler et al. 2024</a> also demonstrate how LLMs can help find a middle ground in political discourses. Similarly, <a href="https://www.pnas.org/doi/10.1073/pnas.2311627120">Argyle et al. 2023</a> document LLMs' ability to elevate the quality of political discourse. These findings illustrate how LLM influence can positively contribute to democratic processes. Future research should identify the conditions that determine whether LLM influence proves beneficial or detrimental. </li>
						</ul>
					</p>
                    <p><strong>Third, we need to explore better policy and value decisions for AI development.</strong> <ul>
					<li><strong>Should we pursue AI political neutrality?</strong> The potential influence of LLMs on democratic processes raises fundamental questions about AI development trajectories. Many advocate for political neutrality in LLMs, assuming these systems should avoid any political leaning [<a href="https://www.mdpi.com/2076-0760/12/3/148">Rozado 2023</a>, <a href="https://arxiv.org/pdf/2410.09978">Vijay et al. 2024</a>]. However, political neutrality is not the only potential goal. One salient alternative is that LLMs should be as factually accurate as possible. <a href="https://www.nature.com/articles/s41586-024-07942-8">Mosleh et al. (2024)</a> showed that liberals tend to share more factually accurate content online than conservatives. This could raise the question of whether pursuing accuracy when training LLMs might contribute to the appearance of models having political leanings. For example, should the LLM represent the scientific consensus regarding the existence of human-caused climate change, or should it equally present arguments for and against human-caused climate change in the name of balance? Even if one does choose to pursue political balance, defining neutrality presents significant challenges, as human perspectives inherently carry bias [<a href="https://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view/FAQ#There%27s_no_such_thing_as_objectivity">Wikipedia NPOV policy</a>, <span style="font-family: cursive">"view from nowhere"</span> from <a href="https://en.wikipedia.org/wiki/Thomas_Nagel">Thomas Nagel</a>'s phrase]. If we define neutrality as what both sides see as unbiased, the media theory [<a href="https://www.tandfonline.com/doi/full/10.1080/15205436.2015.1051234">Perloff 2015</a>] suggests that achieving true neutrality on sensitive topics may be fundamentally impossible. Given this, pursuing LLM political neutrality might be an unattainable goal. Moreover, people often gain valuable insights through engaging with different viewpoints. These considerations raise a future question of whether pursuing LLM political neutrality is truly the right path forward.
					</li>
					<li><strong>Should we pursue AI systems with diverse perspectives or pluralistic values?</strong> An alternative approach involves creating AI systems that embrace pluralistic values. A recent paper [<a href="https://arxiv.org/pdf/2402.05070">Sorensen et al. 2024</a>] proposed developing pluralistic AI systems capable of representing diverse human values and perspectives. But, open questions about implementation strategies remain for future research. For instance, as <a href="https://arxiv.org/pdf/2402.05070">Sorensen et al. (2024)</a> noted, implementing distributional pluralism—where AI systems reflect views in proportion to population demographics—would amplify popular opinions, even when potentially harmful or incorrect. Another promising direction involves multi-agent systems [<a href="https://arxiv.org/pdf/2402.12590">Lai et al. 2024</a>, <a href="https://arxiv.org/pdf/2406.15951">Feng et al. 2024</a>]. Such systems, comprising LLMs with diverse viewpoints, could accommodate and support various perspectives. However, significant challenges remain. Users primarily interacting with LLMs sharing specific viewpoints might reinforce existing biases. This approach could potentially be misused to systematically influence public opinion in particular directions. Additionally, LLMs designed to represent different perspectives may vary in effectiveness based on their inherent leanings. These challenges warrant careful investigation to guide AI development decisions.</li>
					<li><strong>How can we determine appropriate policies and values?</strong> Society must systematically explore these policy and value spaces to identify better directions. This requires comprehensive analysis of potential outcomes across different policies using multiple metrics. This process involves establishing robust evaluation frameworks, including appropriate benchmarks. For example, <a href="https://arxiv.org/pdf/2402.05070">Sorensen et al. (2024)</a> have formalized pluralistic values and proposed corresponding evaluation metrics. Moreover, we need to develop stronger empirical evidence and embrace evidence-based approaches [<a href="https://understanding-ai-safety.org/">Bommasani et al. 2024</a>].</li>
					</ul>
                    </p>
                    <p><strong>Fourth, we need to explore and develop various techniques including safety methods to mitigate or alter LLM political leanings.</strong><ul>
					<li><strong>What methods effectively mitigate or alter LLMs' political leanings?</strong> Alongside exploring better policies and values, we must investigate technical solutions to mitigate or alter LLMs' political leanings. If we decide to pursue political neutrality, what methods should we use? How should we approach cases where we decide to have multiple LLMs with different political leanings? One promising approach involves representation control [<a href="https://arxiv.org/pdf/2310.01405">Zou et al. 2023</a>, <a href="https://www.anthropic.com/research/evaluating-feature-steering">Durmus et al. 2024</a>]. Assuming a political neutrality goal, <a href="https://arxiv.org/pdf/2410.24190">Potter et al. (2024)</a> applied the representation engineering developed by <a href="https://arxiv.org/pdf/2310.01405">Zou et al. (2023)</a> to both Llama-3.1-8B and 70B models, examining potential approaches to reduce LLMs’ political leaning. While these results appear promising, many open questions remain for future investigation: Can we develop methods that mitigate or alter political leaning without compromising model capabilities? How do political leanings in AI models relate to other important model characteristics? These fundamental questions require extensive future exploration.</li>
					</ul></p>
                    <p>Recent papers [<a href="https://arxiv.org/pdf/2410.24190">Potter et al. 2024</a>, <a href="https://arxiv.org/pdf/2410.06415">Fisher et al. 2024</a>, <a href="https://www.science.org/doi/abs/10.1126/science.adq1814">Costello et al. 2024</a>] are among the first empirical explorations of how LLM could influence the political beliefs of users, highlighting the importance of investigating how LLMs will reshape our democracy in the future. The future impact of LLMs on democracy remains an open yet crucial question that society must address. While potential risks exist, LLMs also present opportunities to strengthen democratic processes, such as reducing political polarization and facilitating constructive dialogue. The questions discussed above represent essential steps toward harnessing LLMs' potential for democratic benefit. These issues require continued investigation by the research community. Furthermore, we anticipate that LLMs' influence will extend well beyond political discourse. We encourage both the research community and society at large to thoughtfully explore these possibilities and suggested research directions above.</p>
                </div>
            </div>
        </div>

        <br>
        <br><br><br>


        <style>
            body {
                font-family: 'IBM Plex Serif', serif;
            }

            .name {
                font-weight: bold;
                font-size: medium;
            }

            .details {
                font-weight: normal;
                font-size: small;
            }

            .signer ul {
                list-style-type: none;
                padding-left: 0;
                margin: 0;
            }

            .signer li {
                margin-bottom: 12px;
            }
        </style>

    </div>

    <!-- <div id="bottom">
        <div id="footer">
            <div id="footer-text-container">
                <p class="footer-text">
                    For general inquiries, <a class="footer-text" href="mailto:rdi@berkeley.edu">reach us by
                        email.</a>
                </p>
                <p class="footer-text">
                    © UC Regents 2024.
                </p>
            </div>
        </div>
    </div> -->

</body>

</html>
