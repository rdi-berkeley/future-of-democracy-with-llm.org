<!DOCTYPE html>
<html lang="en" class="h-full">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>LLMs' Potential Influences on Our Democracy: Challenges and Opportunities</title>
    <meta name="description" content="">
    <meta name="keywords" content="">
    <meta name="viewport" content="width=1040">
    <link rel="preconnect" href="https://fonts.googleapis.com/">
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="">
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400&amp;display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Serif:wght@400;700&#038;display=swap"
        rel="stylesheet">
    <link href="./assets/css/css2" rel="stylesheet">

    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700;900&display=swap" rel="stylesheet">

    <!-- Twitter Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta property="twitter:domain" content="democracy-with-llm.org">
    <meta property="twitter:url" content="https://future-of-democracy-with-llm.org/">
    <meta name="twitter:title" content="LLMs' Potential Influences on Our Democracy: Challenges and Opportunities">
    <meta name="twitter:description" content="">
    <!--<meta name="twitter:image" content="https://understanding-ai-safety.org/assets/image.png">-->

    <!--
    <script src="./assets/js/splide.min.js"></script>
    <link rel="stylesheet" href="/assets/css/splide.min.css">
    -->
    <script src="./assets/js/61c2f15e92f56eaa354c18452db280ac.js"></script>
    <link rel="stylesheet" href="./assets/css/style.css">
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            new Splide('.splide').mount();
        });
    </script>
</head>

<body>

    <!-- <div style="background-color: #003262">
        <div class="constraint">
            <div id="header">
                <div id="logo-container" style="padding-top: 1rem;">
                    <a href="./" aria-label="Berkeley Logo">
                        <img src="./assets/logo-berkeley-gold.svg" width="200" aria-label="Berkeley Logo">
                    </a>
                </div>
            </div>
        </div>
    </div> -->

    <div style="border-bottom:3px solid black;">
        <div class="constraint">
            <div style="text-align: center;">
                <h1>
                    LLMs' Potential Influences on Our Democracy: Challenges and Opportunities
                </h1>
                <!--<p>
                    Yujin&nbsp;Potter<sup>†1</sup>, Sanjeev&nbsp;Arora<sup>3</sup>,
                    Yejin&nbsp;Choi<sup>4</sup>, Dawn&nbsp;Song<sup>†1</sup>
                </p>
                <p>
                    <sup>1</sup>UC&nbsp;Berkeley
                    <sup>3</sup>Princeton&nbsp;University
                    <sup>4</sup>University&nbsp;of&nbsp;Washington
                    <sup>5</sup>Harvard&nbsp;University
                    <sup>6</sup>Institute&nbsp;for&nbsp;Advanced&nbsp;Study
                    <sup>7</sup>Cornell&nbsp;University
                    <sup>8</sup>McGill&nbsp;University
                    <sup>9</sup>INRIA
                    <sup>10</sup>Brown&nbsp;University -->
                    <!-- <br> -->
                    <!-- <sup>*</sup>Corresponding author  </p>-->
                
                <!--<p style="font-size: 0.85em;">
                    <sup>*</sup>Several authors have unlisted affiliations in addition to their listed university
                    affiliation. This piece solely reflects the authors' personal views and not those of any affiliated
                    organizations, including the listed universities.
                </p> -->
            </div>
        </div>
    </div>

    <style>
        .main-text div {
            margin-left: 3vw;
            margin-right: 3vw;
            margin-top: 45px;
            font-size: 16px;
            line-height: 24px;
        }

        @media (min-width: 1500px) {
            .main-text div {
                margin-left: 7vw;
                margin-right: 7vw;
            }
        }
    </style>

    <div class="main-text">
        <div class="features-wrapper constraint">
            <div class="row">
                <div class="col-1"></div>
                <div class="col-12 mt-5 paragraph-spacing">
                    <p>As the capabilities of large language models (LLMs) rapidly grow over time, understanding their societal impacts has become increasingly crucial. Accordingly, the potential influence of LLMs on political discourse has emerged as a new critical area. Our recent paper [<a
                    href="https://arxiv.org/abs/2410.24190">Potter et al. 2024</a>] examines LLMs’ political leaning and takes an initial step to explore how LLMs could influence voters through political conversations, using various experiments in the context of presidential elections. Our findings raise important questions that require further investigation. We also outline key research areas to ensure that LLMs can positively contribute to democratic processes and society.</p>
                    <p><h3>LLMs’ Left-of-Center Outputs</h3>
					Our research [<a
                    href="https://arxiv.org/abs/2410.24190">Potter et al. 2024</a>] first examined LLMs' political leanings in multiple scenarios: a voting simulation, LLMs' comments on candidate policies, and interactive political discourse with humans in the context of the U.S. presidential election. These experiments consistently demonstrated LLMs' preference for the Democratic nominee (Biden) and his policies over the Republican nominee (Trump). In particular, during political discussions with humans, LLMs exhibited this political leaning regardless of their human interlocutors' political stances, despite the well-known tendency of LLMs to exhibit sycophancy bias. Our findings add to the growing literature documenting LLMs’ left-of-center, Democratic political views. Recently a number of other researchers have also identified LLMs’ political leanings in various downstream applications. For example, <a
                    href="https://arxiv.org/pdf/2410.09978">Vijay et al. 2024</a> showed that news summarization generated by LLMs tends to highlight liberal perspectives. Similarly, <a
                    href="https://aclanthology.org/2023.acl-long.656/">Feng et al. 2023</a> demonstrated that this left-leaning can also affect their performance in detecting misinformation and hate speech, with varying sensitivity depending on the political orientation of the source material. These studies including ours demonstrate LLMs’ consistent political leanings across diverse scenarios.
					</p>
                    <p><h3>LLMs’ Influence on Users’ Political Stances</h3>Given the documented left-leaning tendencies of LLMs, society has begun discussing their potential influence on users’ political stances and democracy more broadly. However, their impact on democracy, particularly how LLM interactions can affect people's political stances, has remained largely unexplored. Our paper, along with recent studies, has examined whether direct interactions with LLMs can influence people's political stances.</p>
                    <p>Our study [<a
                    href="https://arxiv.org/abs/2410.24190">Potter et al. 2024</a>] demonstrated that even brief interactions with LLMs (which were simply asked to provide answers and comments regarding Biden and Trump's policies without any persuasive intent) could shift voters' choices in the presidential election context, where individuals typically hold strong opinions. A concurrent study [<a href="https://arxiv.org/pdf/2410.06415">Fisher et al, 2024</a>] showed that radical right or left-leaning LLMs can influence human decisions on unfamiliar political topics through short-term conversations, illustrating how LLMs' explicit political leanings can sway humans. Additional research has explored intentional persuasion by LLMs designed to promote specific political positions or spread misinformation [<a href="https://www.anthropic.com/news/measuring-model-persuasiveness">Anthropic persuasion report. 2024</a>, <a href="https://academic.oup.com/pnasnexus/article/3/2/pgae034/7610937">Goldstein et al. 2024</a>]. However, several caveats apply to these studies. The research, including ours, was conducted in simulated settings, which may limit the transferability of results to real-world scenarios. Moreover, these findings’ generalizability and how various factors affect them, including prompt sensitivities, require further exploration.</p>
                    <p><h3>The Path Forward and Future Research Questions</h3> The observed left-leaning tendencies of LLMs and their influence on people's political stances raise many important yet challenging questions that need to be addressed for the benefit of society. We highlight key questions that merit further investigation by both the community and society.</p>
					
					<p>
					<strong>First, we need more comprehensive model evaluation and analysis.</strong>
						<ul>
                        <li><strong>When and how do LLMs manifest their political leaning?</strong></li> While there are a number of studies that have identified LLMs' left political leaning, their outputs can vary significantly depending on prompts and context. For example, <a href="https://arxiv.org/pdf/2402.16786">Röttger et al.</a> demonstrated that LLMs' political answers can change based on how questions are framed. This suggests we need to study their political leanings across various scenarios and contexts. Moreover, as LLM applications become more prevalent in society, it's crucial to understand which specific application scenarios could exhibit these left-leaning tendencies.
                        <li><strong>What causes their left-leaning tendencies?</strong> The complexity of model development makes it difficult to pinpoint the exact causes of this political leaning. One likely factor is the training data itself, which primarily consists of modern web content that may skew liberal. Additionally, our research [<a href="https://arxiv.org/abs/2410.24190">Potter et al. 2024</a>], along with other studies [<a href="https://proceedings.mlr.press/v202/santurkar23a.html">Santurkar et al. 2023</a>, <a href="https://arxiv.org/pdf/2402.05070">Sorensen et al. 2024</a>], suggests that the post-training process may have amplified these political leanings. Specifically, it was shown that instruction-tuned models demonstrate stronger left-leaning tendencies compared to their base versions. This suggests that the current post-training process amplifies the manifestation of its political leaning. However, the specific aspects of the process that cause this amplification remain unclear and require further investigation. Measuring these contributing factors is an important step for fully understanding the models' political leanings. </li>
						</ul></p>
					<p>
						<h3>Second, we need to better understand LLMs’ influence on users and their impacts on democracy.</h3>
						<ul>
                        <li><strong>When and how can LLMs influence users and our democracy and society?</strong></li> While our paper takes a step forward in investigating this question, several important questions remain unsolved. First, how well will our findings transfer to the real world? Currently, there has been no research assessing LLMs' impacts on people's political stance in the wild. This calls for extensive further research, including field experiments, to more accurately evaluate these effects. Second, although significant changes in people's political stance were observed after LLM intervention [<a
						href="https://arxiv.org/abs/2410.24190">Potter et al. 2024</a>], these effects might be time-dependent. For example, political science literature [<a href="https://www.cambridge.org/core/journals/american-political-science-review/article/abs/minimal-persuasive-effects-of-campaign-contact-in-general-elections-evidence-from-49-field-experiments/753665A313C4AB433DBF7110299B7433">Kalla et al. 2017</a>] suggests that even when voter persuasion is initially effective, its impact tends to decay over time. Whether the LLM political sway effect will persist is another interesting question for future research. Third, we need to explore how the deployment of LLMs in society will influence users. Understanding which application scenarios will have greater or lesser societal impact will be crucial. Further studies are needed to understand the conditions under which LLMs influence and sway people.
                        <li><strong>Why can LLMs influence users and our democracy and society?</strong> The answers to these questions remain largely unclear. LLM interactions involve many characteristics, including not only political leanings but also other factors including helpfulness. Understanding LLMs' influence on democracy requires exploring whether each factor can affect people's political stances and determining which factors have stronger or weaker effects. For example, <a href="https://arxiv.org/pdf/2410.06415">Fisher et al.</a> found that LLMs with radical political leanings were effective in swaying people's views on politically less sensitive topics. Similarly, we can investigate whether traits like truthfulness or helpfulness influence people's perspectives. Untangling these various factors to identify the precise causes of LLMs' political influence remains challenging and requires further exploration. </li>
						<li><strong>Should LLM influence on democracy be always viewed negatively?</strong> One might think that LLMs influencing human stances should never occur. However, there is evidence that LLMs could help improve the quality of our democracy. <a href="https://www.science.org/doi/abs/10.1126/science.adq1814">Costello et al.</a> show that LLMs can help durably reduce conspiracy beliefs of people. <a href="https://www.science.org/doi/10.1126/science.adq2852">Tessler et al.</a> also demonstrate how LLMs can help find a middle ground in political discourses. Similarly, Argyle et al. find that LLMs can increase the political conversation quality. These findings show how LLM influence on humans can work positively in democratic processes. Further research should explore the conditions under which LLM influence operates positively or negatively.</li>
						</ul>
					</p>
                    <p><strong>Third, we need to explore better policy and value decisions for AI development.</strong> <ul>
					<li><strong>Should we pursue AI political neutrality?</strong>  The potential influence of LLMs on democratic processes raises important questions about AI development trajectories. Currently, many advocate for political neutrality in LLMs, with the assumption that LLMs shouldn’t exhibit any political leaning [<a href="https://www.mdpi.com/2076-0760/12/3/148">Rozado 2023</a>, <a href="https://arxiv.org/pdf/2410.09978">Vijay et al. 2024</a>]. However,  defining neutrality itself presents significant challenges, as all people have inherent biases [<a href="https://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view/FAQ#There%27s_no_such_thing_as_objectivity">Wikipedia NPOV policy</a>, "view from nowhere" from <a href="https://en.wikipedia.org/wiki/Thomas_Nagel">Thomas Nagel</a>'s phrase]. If we define neutrality as what both sides see as unbiased, the media theory [<a href="https://www.tandfonline.com/doi/full/10.1080/15205436.2015.1051234">Perloff, 2015</a>] suggests that achieving true neutrality on sensitive topics may be fundamentally impossible. Given this, pursuing LLM political neutrality might be an unattainable goal. Moreover, people often gain valuable insights and engagement through exchanging perspectives with entities holding different viewpoints. Further research should investigate whether pursuing LLM political neutrality is the right path forward.
					</li>
					<li><strong>Should we pursue AI systems with different perspectives or pluralistic values?</strong> An alternative approach is to develop AI systems with pluralistic values. A recent paper [<a href="https://arxiv.org/pdf/2402.05070">Sorensen et al. 2024</a>] proposed developing pluralistic AI systems capable of representing diverse human values and perspectives. This approach raises another open question regarding how to operationalize pluralistic AI systems. For instance, as [<a href="https://arxiv.org/pdf/2402.05070">Sorensen et al. 2024</a>] pointed out, implementing distributional pluralism—where AI systems reflect views in proportion to population demographics—would lead to more frequent generation of popular opinions, even when these opinions might be harmful or incorrect. Another promising way expands this problem into multi-agent systems [<a href="https://arxiv.org/pdf/2402.12590">Lai et al. 2024</a>]. A multi-agent system comprising LLMs with diverse viewpoints can accommodate and support various perspectives. However, significant challenges remain. If users primarily interact with LLMs sharing a specific viewpoint, certain stereotypes and biases might become more entrenched. There's also a risk of this approach being misused to systematically sway people in particular directions. Additionally, the performance of LLMs designed to represent different perspectives may vary depending on their inherent leaning. These challenges require further exploration and investigation to inform better AI development decisions.</li>
					<li><strong>How can we determine appropriate policies and values?</strong>Society needs to systematically explore these policy and value spaces to identify better directions. This requires a comprehensive study of potential consequences across different policies using various metrics. This will also involve establishing evaluation frameworks including a benchmark. For example, [<a href="https://arxiv.org/pdf/2402.05070">Sorensen et al. 2024</a>] formalized pluralistic values and proposed pluralistic benchmarks for model evaluation. Moreover, we need to develop better scientific evidence and take an evidence-based approach [<a href="https://understanding-ai-safety.org/">Bommasani et al. 2024</a>].</li>
					</ul>
                    </p>
                    <p><strong>Fourth, we need to explore and develop various techniques including safety methods to mitigate LLM political leanings.</strong><ul>
					<li><strong>What are effective methods to mitigate LLMs’ political leanings?</strong>  Along with exploring better policies and values, we must investigate technical approaches to mitigate LLMs' political leanings. One promising approach involves representation control [<a href="https://arxiv.org/pdf/2310.01405">Zou et al. 2023</a>, <a href="https://www.anthropic.com/research/evaluating-feature-steering">Durmus et al. 2024</a>]. Our paper [<a href="https://arxiv.org/abs/2410.24190">Potter et al. 2024</a>] applied the representation engineering developed by <a href="https://arxiv.org/pdf/2310.01405">Zou et al</a> to both Llama-3.1-8B and 70B models, to examine the potential for reducing political leaning with representation control. While these results show promise, they also raise several critical questions for future investigation: Can we develop methods that reduce political leaning without compromising model capabilities? Is political neutrality in AI models potentially misaligned with other important objectives? These fundamental questions require extensive future exploration.</li>
					</ul></p>
                    <p>How LLMs will shape the future of democracy remains an unknown yet critical question that the society must explore. While potential risks exist, LLMs also offer promising opportunities to enhance democratic processes, such as reducing political conflicts and helping find common ground. The questions discussed above represent necessary steps toward harnessing LLMs positively for democracy. These require further investigation by the community. Moreover, we anticipate that LLMs' influence will extend well beyond the political domain. We encourage the community and society to explore these possibilities to maximize the potential benefits of LLMs, reducing the risks.</p>
                </div>
            </div>
        </div>

        <br>
        <br><br><br>


        <style>
            body {
                font-family: 'IBM Plex Serif', serif;
            }

            .name {
                font-weight: bold;
                font-size: medium;
            }

            .details {
                font-weight: normal;
                font-size: small;
            }

            .signer ul {
                list-style-type: none;
                padding-left: 0;
                margin: 0;
            }

            .signer li {
                margin-bottom: 12px;
            }
        </style>

    </div>

    <!-- <div id="bottom">
        <div id="footer">
            <div id="footer-text-container">
                <p class="footer-text">
                    For general inquiries, <a class="footer-text" href="mailto:rdi@berkeley.edu">reach us by
                        email.</a>
                </p>
                <p class="footer-text">
                    © UC Regents 2024.
                </p>
            </div>
        </div>
    </div> -->

</body>

</html>