<!DOCTYPE html>
<html lang="en" class="h-full">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>LLMs' Potential Influences on Our Democracy: Challenges and Opportunities</title>
    <meta name="description" content="">
    <meta name="keywords" content="">
    <meta name="viewport" content="width=1040">
    <link rel="preconnect" href="https://fonts.googleapis.com/">
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="">
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400&amp;display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Serif:wght@400;700&#038;display=swap"
        rel="stylesheet">
    <link href="./assets/css/css2" rel="stylesheet">

    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700;900&display=swap" rel="stylesheet">

    <!-- Twitter Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta property="twitter:domain" content="democracy-with-llm.org">
    <meta property="twitter:url" content="https://future-of-democracy-with-llm.org/">
    <meta name="twitter:title" content="LLMs' Potential Influences on Our Democracy: Challenges and Opportunities">
    <meta name="twitter:description" content="">
    <!--<meta name="twitter:image" content="https://understanding-ai-safety.org/assets/image.png">-->

    <!--
    <script src="./assets/js/splide.min.js"></script>
    <link rel="stylesheet" href="/assets/css/splide.min.css">
    -->
    <script src="./assets/js/61c2f15e92f56eaa354c18452db280ac.js"></script>
    <link rel="stylesheet" href="./assets/css/style.css">
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            new Splide('.splide').mount();
        });
    </script>
</head>

<body style="background-color: #fffbe6;">

    <!-- <div style="background-color: #003262">
        <div class="constraint">
            <div id="header">
                <div id="logo-container" style="padding-top: 1rem;">
                    <a href="./" aria-label="Berkeley Logo">
                        <img src="./assets/logo-berkeley-gold.svg" width="200" aria-label="Berkeley Logo">
                    </a>
                </div>
            </div>
        </div>
    </div> -->

    <div style="border-bottom:3px solid black;">
        <div class="constraint">
            <div style="text-align: center;">
                <h1>
                    LLMs' Potential Influences on Our Democracy: Challenges and Opportunities
                </h1>
                <!--<p>
                    Yujin&nbsp;Potter<sup>†1</sup>, Sanjeev&nbsp;Arora<sup>3</sup>,
                    Yejin&nbsp;Choi<sup>4</sup>, Dawn&nbsp;Song<sup>†1</sup>
                </p>
                <p>
                    <sup>1</sup>UC&nbsp;Berkeley
                    <sup>3</sup>Princeton&nbsp;University
                    <sup>4</sup>University&nbsp;of&nbsp;Washington
                    <sup>5</sup>Harvard&nbsp;University
                    <sup>6</sup>Institute&nbsp;for&nbsp;Advanced&nbsp;Study
                    <sup>7</sup>Cornell&nbsp;University
                    <sup>8</sup>McGill&nbsp;University
                    <sup>9</sup>INRIA
                    <sup>10</sup>Brown&nbsp;University -->
                    <!-- <br> -->
                    <!-- <sup>*</sup>Corresponding author  </p>-->
                
                <!--<p style="font-size: 0.85em;">
                    <sup>*</sup>Several authors have unlisted affiliations in addition to their listed university
                    affiliation. This piece solely reflects the authors' personal views and not those of any affiliated
                    organizations, including the listed universities.
                </p> -->
            </div>
        </div>
    </div>

    <style>
        .main-text div {
            margin-left: 3vw;
            margin-right: 3vw;
            margin-top: 45px;
            font-size: 18px;
            line-height: 26px;
        }

        @media (min-width: 1500px) {
            .main-text div {
                margin-left: 7vw;
                margin-right: 7vw;
            }
        }
    </style>

    <div class="main-text">
        <div class="features-wrapper constraint">
            <div class="row">
                <div class="col-1"></div>
                <div class="col-12 mt-5 paragraph-spacing">
                    <p>As large language models (LLMs) continue to advance at a remarkable pace, understanding their societal implications has become increasingly vital. In particular, the potential influence of LLMs on political discourse has emerged as a critical area of study. Our recent paper [<a
                    href="https://arxiv.org/abs/2410.24190">Potter et al. 2024</a>] investigated LLMs' political leanings and took an initial step to explore how these systems might influence voters through political conversations, particularly in the context of presidential elections. Our findings raise important questions that need further exploration. We also identify key research directions to ensure LLMs can constructively contribute to democratic processes and society.</p>
                    <p><h3>LLMs’ Left-of-Center Outputs</h3>
					Our research [<a
                    href="https://arxiv.org/abs/2410.24190">Potter et al. 2024</a>] first examined LLMs' political leanings in multiple scenarios: a voting simulation, LLMs' comments on candidate policies, and interactive political discourse with users in the context of the U.S. presidential election. These experiments consistently demonstrated LLMs' preference for the Democratic nominee (Biden) and his policies over the Republican nominee (Trump). In particular, during political discussions with humans, LLMs exhibited this political leaning regardless of their human interlocutors' political stances, despite the well-known tendency of LLMs to exhibit sycophancy bias. Our findings complement growing literature showing LLMs' left-of-center, Democratic political perspectives. Recent studies have documented similar patterns in various applications. For example, <a
                    href="https://arxiv.org/pdf/2410.09978">Vijay et al. 2024</a> showed that LLM-generated news summaries tend to highlight liberal perspectives. Similarly, <a
                    href="https://aclanthology.org/2023.acl-long.656/">Feng et al. 2023</a> showed that this left-leaning tendency affects their ability to detect misinformation and hate speech, with varying sensitivity based on the political orientation of source materials. These studies, including ours, demonstrate LLMs’ consistent political leanings across diverse contexts.
					</p>
                    <p><h3>LLMs’ Influence on Users’ Political Stances</h3>Given these documented left-leaning tendencies, society has begun discussing LLMs' potential impact on users' political views and democratic processes more broadly. However, their effect on democracy, particularly how interactions with LLMs might shape users’ political perspectives, remains largely unexplored. Our paper, alongside recent research, examines whether direct LLM interactions can influence users’ political viewpoints.</p>
                    <p>Our study [<a
                    href="https://arxiv.org/abs/2410.24190">Potter et al. 2024</a>] demonstrated that even brief interactions with LLMs (which were simply asked to provide answers and comments regarding Biden and Trump's policies without any persuasive intent) could shift voters' choices in the presidential election context, where individuals typically hold firm opinions. However, several caveats apply to this study. For example, because this experiment was conducted in controlled settings, it may potentially limit real-world applicability. Moreover, these findings’ generalizability and the influence of various factors, including prompt variations and experiment timing, warrant additional investigation. A parallel study [<a href="https://arxiv.org/pdf/2410.06415">Fisher et al, 2024</a>] demonstrated that LLMs programmed with extreme right or left-wing viewpoints can influence human decisions on unfamiliar political topics through brief conversations, illustrating how LLMs' explicit political leanings can affect users’ political stances. Additional research has explored intentional persuasion by LLMs designed to promote specific political positions or spread misinformation [<a href="https://www.anthropic.com/news/measuring-model-persuasiveness">Anthropic persuasion report. 2024</a>, <a href="https://academic.oup.com/pnasnexus/article/3/2/pgae034/7610937">Goldstein et al. 2024</a>]. These studies describe how LLMs could influence our democracy, highlighting the necessity of further research.</p>
                    <p><h3>The Path Forward and Future Research Questions</h3> The observed left-leaning tendencies of LLMs and their influence on users’ political perspectives raise critical questions that society must address. We highlight four key areas requiring further investigation by both researchers and society at large.</p>
					
					<p>
					<strong>First, we need more comprehensive model evaluation and analysis.</strong>
						<ul>
                        <li><strong>When and how can LLMs shape users' views and affect our democratic processes?</strong></li> While many studies have identified LLMs' left-leaning tendencies, their responses can vary based on prompts and context. For example, <a href="https://arxiv.org/pdf/2402.16786">Röttger et al.</a> demonstrated that LLMs' political answers can shift depending on question framing. This suggests the need for examining LLMs’  political leanings across diverse scenarios. Additionally, as LLM applications become more widespread, understanding which specific use cases might exhibit these left-leaning tendencies becomes crucial.
                        <li><strong>What causes LLMs’ left-leaning tendencies?</strong> The complexity of model development makes identifying precise causes challenging. Training data composition likely plays a role, as it primarily consists of modern web content that may skew liberal. Additionally, our research [<a href="https://arxiv.org/abs/2410.24190">Potter et al. 2024</a>], along with other studies [<a href="https://proceedings.mlr.press/v202/santurkar23a.html">Santurkar et al. 2023</a>, <a href="https://arxiv.org/pdf/2402.05070">Sorensen et al. 2024</a>], indicates that post-training processes may amplify these political leanings. Specifically, instruction-tuned models show stronger left-leaning tendencies compared to their base versions, suggesting that current post-training methods increase political leanings. However, the specific aspects driving this amplification remain unclear and require further investigation. Quantifying these contributing factors represents an essential step toward understanding the models' political leanings. </li>
						</ul></p>
					<p>
						<h3>Second, we need to deepen our understanding of LLMs' influence on users and their impact on democracy.</h3>
						<ul>
                        <li><strong>When and how can LLMs influence users and our democracy and society?</strong></li> While our paper takes a step forward in investigating this question, several crucial questions remain unresolved. Foremost, how effectively will our findings translate to real-world scenarios? Currently, no research has assessed LLMs' impact on users’ political perspectives in the wild. This gap calls for extensive field experiments to evaluate these effects more accurately. Second, although significant changes in people's political stance were observed after LLM intervention [<a
						href="https://arxiv.org/abs/2410.24190">Potter et al. 2024</a>], these effects might be time-dependent. For example, political science literature [<a href="https://www.cambridge.org/core/journals/american-political-science-review/article/abs/minimal-persuasive-effects-of-campaign-contact-in-general-elections-evidence-from-49-field-experiments/753665A313C4AB433DBF7110299B7433">Kalla et al. 2017</a>] suggests that even successful voter persuasion tends to diminish over time. Whether LLM-induced political shifts persist presents another compelling avenue for future research. Furthermore, we must explore how LLM deployment in various contexts will affect users. Understanding which applications carry greater or lesser influence on users will prove essential. Additional research should examine the conditions under which LLMs most significantly influence public opinion.
                        <li><strong>Why can LLMs shape users' views and affect our democratic processes?</strong> The mechanisms behind these effects remain largely undefined. LLM interactions encompass numerous characteristics beyond political leanings, including elements like helpfulness and engagement style. Understanding their influence requires exploring how each factor might affect users’ political perspectives and determining their relative impact. For example, <a href="https://arxiv.org/pdf/2410.06415">Fisher et al.</a> discovered that LLMs expressing extreme political views effectively swayed user opinions on less politically charged topics. Similarly, we need to investigate which other aspects (like truthfulness or helpfulness) of LLM interactions can influence people's viewpoints. Disentangling these various factors to identify the precise mechanisms of LLMs' political influence presents an ongoing challenge requiring sustained research. </li>
						<li><strong>Should we always view LLM influence on democracy as problematic?</strong>  While some might argue that any LLM influence on human perspectives is inherently concerning, recent work suggests these systems could enhance democratic processes. <a href="https://www.science.org/doi/abs/10.1126/science.adq1814">Costello et al.</a> demonstrate that LLMs can effectively reduce individuals' belief in conspiracy theories. <a href="https://www.science.org/doi/10.1126/science.adq2852">Tessler et al.</a> also demonstrate how LLMs can help find a middle ground in political discourses. Similarly, <a href="https://www.pnas.org/doi/10.1073/pnas.2311627120">Argyle et al.</a> document LLMs' ability to elevate the quality of political discourse. These findings illustrate how LLM influence can positively contribute to democratic processes. Future research should identify the conditions that determine whether LLM influence proves beneficial or detrimental.</li>
						</ul>
					</p>
                    <p><strong>Third, we need to explore better policy and value decisions for AI development.</strong> <ul>
					<li><strong>Should we pursue AI political neutrality?</strong>  The potential influence of LLMs on democratic processes raises fundamental questions about AI development trajectories. Many advocate for political neutrality in LLMs, assuming these systems should avoid any political leaning [<a href="https://www.mdpi.com/2076-0760/12/3/148">Rozado 2023</a>, <a href="https://arxiv.org/pdf/2410.09978">Vijay et al. 2024</a>]. However, defining neutrality presents significant challenges, as human perspective inherently carries bias [<a href="https://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view/FAQ#There%27s_no_such_thing_as_objectivity">Wikipedia NPOV policy</a>, "view from nowhere" from <a href="https://en.wikipedia.org/wiki/Thomas_Nagel">Thomas Nagel</a>'s phrase]. If we define neutrality as what both sides see as unbiased, the media theory [<a href="https://www.tandfonline.com/doi/full/10.1080/15205436.2015.1051234">Perloff, 2015</a>] suggests that achieving true neutrality on sensitive topics may be fundamentally impossible. Given this, pursuing LLM political neutrality might be an unattainable goal. Moreover, people often gain valuable insights through engaging with different viewpoints. These considerations raise a future question of whether pursuing LLM political neutrality is truly the right path forward.
					</li>
					<li><strong>Should we pursue AI systems with diverse perspectives or pluralistic values?</strong> An alternative approach involves creating AI systems that embrace pluralistic values. A recent paper [<a href="https://arxiv.org/pdf/2402.05070">Sorensen et al. 2024</a>] proposed developing pluralistic AI systems capable of representing diverse human values and perspectives. But, open questions about implementation strategies remain for future research. For instance, as [<a href="https://arxiv.org/pdf/2402.05070">Sorensen et al. 2024</a>] noted, implementing distributional pluralism—where AI systems reflect views in proportion to population demographics—would amplify popular opinions, even when potentially harmful or incorrect. Another promising direction involves multi-agent systems [<a href="https://arxiv.org/pdf/2402.12590">Lai et al. 2024</a>]. Such systems, comprising LLMs with diverse viewpoints, could accommodate and support various perspectives. However, significant challenges remain. Users primarily interacting with LLMs sharing specific viewpoints might reinforce existing biases. This approach could potentially be misused to systematically influence public opinion in particular directions. Additionally, LLMs designed to represent different perspectives may vary in effectiveness based on their inherent leanings. These challenges warrant careful investigation to guide AI development decisions.</li>
					<li><strong>How can we determine appropriate policies and values?</strong>Society must systematically explore these policy and value spaces to identify better directions. This requires comprehensive analysis of potential outcomes across different policies using multiple metrics. This process involves establishing robust evaluation frameworks, including appropriate benchmarks. For example, [<a href="https://arxiv.org/pdf/2402.05070">Sorensen et al. 2024</a>] has formalized pluralistic values and proposed corresponding evaluation metrics. Moreover, we need to develop stronger empirical evidence and embrace evidence-based approaches [<a href="https://understanding-ai-safety.org/">Bommasani et al. 2024</a>].</li>
					</ul>
                    </p>
                    <p><strong>Fourth, we need to explore and develop various techniques including safety methods to mitigate LLM political leanings.</strong><ul>
					<li><strong>What methods effectively mitigate LLMs' political leanings?</strong>  Alongside exploring better policies and values, we must investigate technical solutions to address LLMs' political leanings. One promising approach involves representation control [<a href="https://arxiv.org/pdf/2310.01405">Zou et al. 2023</a>, <a href="https://www.anthropic.com/research/evaluating-feature-steering">Durmus et al. 2024</a>]. Our paper [<a href="https://arxiv.org/abs/2410.24190">Potter et al. 2024</a>] applied the representation engineering developed by <a href="https://arxiv.org/pdf/2310.01405">Zou et al</a> to both Llama-3.1-8B and 70B models, examining potential approaches to reduce LLMs’ political leaning. While these results appear promising, many open questions remain for future investigation: Can we develop methods that reduce political leaning without compromising model capabilities? Is political neutrality in AI models potentially misaligned with other important objectives? These fundamental questions require extensive future exploration.</li>
					</ul></p>
                    <p>Our paper [<a href="https://arxiv.org/abs/2410.24190">Potter et al. 2024</a>] presents the first empirical exploration of how LLM conversations could influence voter choices, which may significantly impact our democracy. The future impact of LLMs on democracy remains an open yet crucial question that society must address. While potential risks exist, LLMs also present opportunities to strengthen democratic processes, such as reducing political polarization and facilitating constructive dialogue. The questions discussed above represent essential steps toward harnessing LLMs' potential for democratic benefit. These issues require continued investigation by the research community. Furthermore, we anticipate that LLMs' influence will extend well beyond political discourse. We encourage both the research community and society at large to explore these possibilities thoughtfully, maximizing potential benefits while minimizing risks.</p>
                </div>
            </div>
        </div>

        <br>
        <br><br><br>


        <style>
            body {
                font-family: 'IBM Plex Serif', serif;
            }

            .name {
                font-weight: bold;
                font-size: medium;
            }

            .details {
                font-weight: normal;
                font-size: small;
            }

            .signer ul {
                list-style-type: none;
                padding-left: 0;
                margin: 0;
            }

            .signer li {
                margin-bottom: 12px;
            }
        </style>

    </div>

    <!-- <div id="bottom">
        <div id="footer">
            <div id="footer-text-container">
                <p class="footer-text">
                    For general inquiries, <a class="footer-text" href="mailto:rdi@berkeley.edu">reach us by
                        email.</a>
                </p>
                <p class="footer-text">
                    © UC Regents 2024.
                </p>
            </div>
        </div>
    </div> -->

</body>

</html>
